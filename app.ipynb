{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ei2fWNH750h"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Install required libraries\n",
        "subprocess.check_call([\"pip\", \"install\", \"torch>=1.11.0\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"transformers>=4.31.0\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"diffusers>=0.14.0\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"librosa\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"accelerate>=0.20.1\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"gradio>=3.35.2\"])\n",
        "subprocess.check_call([\"pip\", \"install\", \"huggingface_hub\"])\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import gradio as gr\n",
        "from functools import lru_cache\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "# Ensure required dependencies are installed\n",
        "def install_missing_packages():\n",
        "    required_packages = {\n",
        "        \"librosa\": None,\n",
        "        \"diffusers\": \">=0.14.0\",\n",
        "        \"gradio\": \">=3.35.2\",\n",
        "        \"huggingface_hub\": None,\n",
        "        \"accelerate\": \">=0.20.1\",\n",
        "        \"transformers\": \">=4.31.0\"\n",
        "    }\n",
        "    for package, version in required_packages.items():\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            package_name = f\"{package}{version}\" if version else package\n",
        "            subprocess.check_call([\"pip\", \"install\", package_name])\n",
        "\n",
        "install_missing_packages()\n",
        "\n",
        "# Get Hugging Face token for authentication\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "else:\n",
        "    raise ValueError(\"HF_TOKEN environment variable not set.\")\n",
        "\n",
        "# Load speech-to-text model (Whisper)\n",
        "speech_to_text = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-tiny\",\n",
        "    return_timestamps=True\n",
        ")\n",
        "\n",
        "# Load Stable Diffusion model for text-to-image\n",
        "text_to_image = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "text_to_image.to(device)\n",
        "text_to_image.enable_attention_slicing()\n",
        "text_to_image.safety_checker = None\n",
        "text_to_image.scheduler = DPMSolverMultistepScheduler.from_config(text_to_image.scheduler.config)\n",
        "\n",
        "# Preprocess audio file into NumPy array\n",
        "def preprocess_audio(audio_path):\n",
        "    try:\n",
        "        audio, sr = librosa.load(audio_path, sr=16000)  # Resample to 16kHz\n",
        "        return np.array(audio, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        return f\"Error in preprocessing audio: {str(e)}\"\n",
        "\n",
        "# Speech-to-text function with long-form transcription support\n",
        "@lru_cache(maxsize=10)\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        audio_array = preprocess_audio(audio_path)\n",
        "        if isinstance(audio_array, str):  # Error message from preprocessing\n",
        "            return audio_array\n",
        "        result = speech_to_text(audio_array)\n",
        "        # Combine text from multiple segments for long-form transcription\n",
        "        transcription = \" \".join(segment[\"text\"] for segment in result[\"chunks\"])\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        return f\"Error in transcription: {str(e)}\"\n",
        "\n",
        "# Text-to-image function\n",
        "@lru_cache(maxsize=10)\n",
        "def generate_image_from_text(text):\n",
        "    try:\n",
        "        image = text_to_image(text, height=256, width=256).images[0]  # Generate smaller images for speed\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        return f\"Error in image generation: {str(e)}\"\n",
        "\n",
        "# Combined processing function\n",
        "def process_audio_and_generate_results(audio_path):\n",
        "    transcription_result = {\"result\": None}\n",
        "    image_result = {\"result\": None}\n",
        "\n",
        "    # Function to run transcription and image generation in parallel\n",
        "    def transcription_thread():\n",
        "        transcription_result[\"result\"] = transcribe_audio(audio_path)\n",
        "\n",
        "    def image_generation_thread():\n",
        "        transcription = transcription_result[\"result\"]\n",
        "        if transcription and \"Error\" not in transcription:\n",
        "            image_result[\"result\"] = generate_image_from_text(transcription)\n",
        "\n",
        "    # Start both tasks in parallel\n",
        "    t1 = threading.Thread(target=transcription_thread)\n",
        "    t2 = threading.Thread(target=image_generation_thread)\n",
        "\n",
        "    t1.start()\n",
        "    t2.start()\n",
        "\n",
        "    t1.join()  # Wait for transcription to finish\n",
        "    t2.join()  # Wait for image generation to finish\n",
        "\n",
        "    transcription = transcription_result[\"result\"]\n",
        "    image = image_result[\"result\"]\n",
        "\n",
        "    if \"Error\" in transcription:\n",
        "        return None, transcription\n",
        "    if isinstance(image, str) and \"Error\" in image:\n",
        "        return None, image\n",
        "\n",
        "    return image, transcription\n",
        "\n",
        "# Gradio interface for speech-to-text\n",
        "speech_to_text_iface = gr.Interface(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload audio file for transcription (WAV/MP3)\"),\n",
        "    outputs=gr.Textbox(label=\"Transcription\"),\n",
        "    title=\"Speech-to-Text Transcription\",\n",
        "    description=\"Upload an audio file to transcribe speech into text.\",\n",
        ")\n",
        "\n",
        "# Gradio interface for voice-to-image\n",
        "voice_to_image_iface = gr.Interface(\n",
        "    fn=process_audio_and_generate_results,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload audio file (WAV/MP3)\"),\n",
        "    outputs=[gr.Image(label=\"Generated Image\"), gr.Textbox(label=\"Transcription\")],\n",
        "    title=\"Voice-to-Image\",\n",
        "    description=\"Upload an audio file to transcribe speech to text and generate an image based on the transcription.\",\n",
        ")\n",
        "\n",
        "# Combined Gradio app\n",
        "iface = gr.TabbedInterface(\n",
        "    interface_list=[speech_to_text_iface, voice_to_image_iface],\n",
        "    tab_names=[\"Speech-to-Text\", \"Voice-to-Image\"]\n",
        ")\n",
        "\n",
        "# Launch Gradio interface\n",
        "iface.launch(debug=True, share=True)"
      ]
    }
  ]
}